# requirements-spark.txt
# Python packages needed for Spark jobs

# Core PySpark (should match Spark version)
pyspark==3.5.0

# Data processing
pandas==2.0.3
numpy==1.24.3
pyarrow==12.0.1

# Kafka integration
kafka-python==2.0.2

# Database connections
psycopg2-binary==2.9.7
sqlalchemy==2.0.21

# Cloud storage
boto3==1.28.57

# Monitoring
py4j==0.10.9.7
prometheus-client==0.17.1

# Data quality
great-expectations==0.17.19

# Utilities
python-dotenv==1.0.0
pyyaml==6.0.1