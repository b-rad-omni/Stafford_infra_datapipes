# docker/spark/log4j2.properties
# Logging configuration for Spark - reduces noise, focuses on important messages

# Set root logger level to WARN to reduce verbosity
rootLogger.level = warn
rootLogger.appenderRef.stdout.ref = console

# Console appender configuration
appender.console.type = Console
appender.console.name = console
appender.console.target = SYSTEM_OUT
appender.console.layout.type = PatternLayout
appender.console.layout.pattern = %d{yy/MM/dd HH:mm:ss} %p %c{1}: %m%n

# Set Spark-specific loggers to appropriate levels
logger.spark.name = org.apache.spark
logger.spark.level = info

logger.spark_sql.name = org.apache.spark.sql
logger.spark_sql.level = warn

logger.kafka.name = org.apache.kafka
logger.kafka.level = warn

logger.streaming.name = org.apache.spark.streaming
logger.streaming.level = info

# Reduce Hadoop noise
logger.hadoop.name = org.apache.hadoop
logger.hadoop.level = error

# Our application logs should be at INFO level
logger.app.name = com.datapipeline
logger.app.level = info